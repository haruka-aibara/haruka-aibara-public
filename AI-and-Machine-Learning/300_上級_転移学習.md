# 転移学習 (難易度レベル: 300)

## 概要
転移学習（Transfer Learning）は、あるドメインで学習した知識を別のドメインに転移させる機械学習の手法です。事前学習済みモデルの特徴抽出能力を活用することで、少ないデータセットでも高い性能を達成できることが特徴です。特に、深層学習において重要な技術として位置づけられています。

## 詳細

### 転移学習の基本概念
転移学習は以下の要素で構成されます：

1. **ソースドメイン（Source Domain）**
   - 事前学習に使用されるデータセット
   - 通常、大規模な汎用データセット（例：ImageNet、Wikipedia）

2. **ターゲットドメイン（Target Domain）**
   - 転移先の特定タスク
   - 通常、小規模な専門データセット

3. **転移学習の種類**
   - ドメイン適応（Domain Adaptation）
   - マルチタスク学習（Multi-task Learning）
   - 知識蒸留（Knowledge Distillation）

### 転移学習の実装手法

#### 1. 特徴抽出器としての利用
```python
import torch
import torchvision.models as models
import torch.nn as nn

# 事前学習済みモデルの読み込み
model = models.resnet50(pretrained=True)

# 特徴抽出器として使用
for param in model.parameters():
    param.requires_grad = False

# 新しい分類層の追加
model.fc = nn.Linear(2048, num_classes)
```

#### 2. ファインチューニング
```python
# 特定の層のみ学習可能に設定
for param in model.layer4.parameters():
    param.requires_grad = True

# 学習率の調整
optimizer = torch.optim.Adam([
    {'params': model.layer4.parameters(), 'lr': 1e-4},
    {'params': model.fc.parameters(), 'lr': 1e-3}
])
```

### 転移学習の最適化手法

#### 1. 学習率の調整
- 事前学習済み層：小さな学習率（1e-4程度）
- 新規追加層：大きな学習率（1e-3程度）

#### 2. バッチ正規化の扱い
```python
# バッチ正規化層の統計量を固定
model.eval()
for m in model.modules():
    if isinstance(m, nn.BatchNorm2d):
        m.eval()
        m.requires_grad = False
```

#### 3. データ拡張
```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
```

### 実践的な応用例

#### 1. 医療画像診断への応用
```python
# 医療画像用の転移学習モデル
class MedicalImageModel(nn.Module):
    def __init__(self, pretrained_model):
        super().__init__()
        self.features = pretrained_model.features
        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
```

#### 2. 自然言語処理への応用
```python
from transformers import BertModel, BertTokenizer

# BERTモデルの転移学習
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# 特定タスク用の分類層を追加
class BertClassifier(nn.Module):
    def __init__(self, bert_model, num_classes):
        super().__init__()
        self.bert = bert_model
        self.classifier = nn.Linear(768, num_classes)
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return self.classifier(outputs[1])
```

### パフォーマンス最適化のポイント

1. **モデル選択**
   - タスクの複雑さに応じたモデルサイズの選択
   - 計算リソースと性能のバランス

2. **ハイパーパラメータの調整**
   - 学習率のスケジューリング
   - バッチサイズの最適化
   - 正則化パラメータの調整

3. **評価指標の選択**
   - タスクに適した評価指標の使用
   - クロスバリデーションの実施

## まとめ

### 重要なポイント
- 転移学習は、限られたデータセットでも高い性能を達成できる
- 適切な事前学習モデルの選択が重要
- ファインチューニングの戦略が性能に大きく影響
- ドメイン適応の手法を理解することが重要

### 次のステップ
1. **高度な転移学習手法の学習**
   - メタ学習（Meta-learning）
   - ドメイン適応の最新手法
   - マルチモーダル転移学習

2. **実践的な応用**
   - 産業分野への適用
   - 研究開発での活用
   - 新しいドメインへの展開

3. **最新の研究動向**
   - 自己教師あり学習との組み合わせ
   - 効率的な転移学習手法の開発
   - ドメイン適応の自動化 
